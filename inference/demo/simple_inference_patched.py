#!/usr/bin/env python3
"""
ä¿®æ”¹åçš„ simple_inference.py - ä½¿ç”¨è¿è¡Œæ—¶è¡¥ä¸æ›¿æ¢å¤„ç†å™¨
"""

import torch
from PIL import Image
import re
import numpy as np
import copy
import argparse
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è¿è¡Œæ—¶è¡¥ä¸æ¨¡å—
from runtime_patch import patch_mllama_processor, with_custom_processor

# åœ¨å¯¼å…¥ transformers ä¹‹å‰åº”ç”¨è¡¥ä¸
print("ğŸ”§ æ­£åœ¨åº”ç”¨è¿è¡Œæ—¶è¡¥ä¸...")
patch_success = patch_mllama_processor()

if not patch_success:
    print("âŒ è¡¥ä¸åº”ç”¨å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
    sys.exit(1)

# ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ transformers
from transformers import StoppingCriteria, StoppingCriteriaList, MllamaForConditionalGeneration, AutoProcessor

parser = argparse.ArgumentParser(description="LLaVA-CoT Simple Inference with Runtime Patching")
parser.add_argument(
    "--model_name_or_path",
    type=str,
    default="Xkev/Llama-3.2V-11B-cot",
    help="Path to the model.",
)
parser.add_argument(
    "--prompt",
    type=str,
    help="Prompt to ask the model.",
)
parser.add_argument(
    "--image_path",
    type=str,
    help="Path to the image.",
)
parser.add_argument(
    "--type",
    type=str,
    default="stage",
    choices=["best_of_N", "sentence", "stage"],
    help="Type of generation to perform.",
)
parser.add_argument(
    "--beam_size",
    type=int,
    default=2,
    help="Number of candidates to generate.",
)
parser.add_argument(
    "--device",
    type=str,
    default="cuda",
    help="Device to use for inference.",
)
args = parser.parse_args()

class StopOnStrings(StoppingCriteria):
    def __init__(self, stop_strings, tokenizer):
        self.stop_strings = stop_strings
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        for stop_string in self.stop_strings:
            if stop_string in generated_text:
                return True
        return False
    
class StopOnPeriod(StoppingCriteria):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        if generated_text.endswith('.'):
            return True
        return False

print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
model_name_or_path = args.model_name_or_path
model = MllamaForConditionalGeneration.from_pretrained(
        model_name_or_path,
        torch_dtype=torch.bfloat16,
        device_map='cpu',
    ).cuda().eval()
device = args.device
processor = AutoProcessor.from_pretrained(model_name_or_path)
kwargs = dict(do_sample=True, max_new_tokens=2048, temperature=0.6, top_p=0.9)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨")

# è¿™é‡Œå¯ä»¥ç»§ç»­æ·»åŠ åŸæœ‰çš„ judge å‡½æ•°å’Œç”Ÿæˆå‡½æ•°
# ç”±äºå­—ç¬¦é™åˆ¶ï¼Œæˆ‘å°†åœ¨ä¸‹ä¸€ä¸ªæ–‡ä»¶ä¸­ç»§ç»­æ·»åŠ å…¶ä½™å‡½æ•°

def judge(image, prompt, outputs, type="summary"):
    # è¿™é‡Œæ˜¯åŸæœ‰çš„ judge å‡½æ•°å®ç°
    # ä¸ºäº†èŠ‚çœç©ºé—´ï¼Œè¿™é‡Œåªæ˜¾ç¤ºå‡½æ•°ç­¾å
    # å®Œæ•´å®ç°è¯·å‚è€ƒåŸå§‹æ–‡ä»¶
    pass

def generate_inner_best_of_N(prompt, image_path, beam_size=2):
    # åŸæœ‰çš„å®ç°
    pass

def generate_inner_sentence_beam(prompt, image_path, beam_size=2):
    # åŸæœ‰çš„å®ç°
    pass

def generate_inner_stage_beam(prompt, image_path, beam_size=2):
    # åŸæœ‰çš„å®ç°
    pass

def generate_inner(prompt, image_path, type="stage", beam_size=2):
    if type == "best_of_N":
        return generate_inner_best_of_N(prompt, image_path, beam_size)
    elif type == "sentence":
        return generate_inner_sentence_beam(prompt, image_path, beam_size)
    elif type == "stage":
        return generate_inner_stage_beam(prompt, image_path, beam_size)
    else:
        raise ValueError("Invalid type. Choose from 'best_of_N', 'sentence', or 'stage'.")

if __name__ == "__main__":
    print("ğŸ¯ å¼€å§‹æ¨ç†...")
    result = generate_inner(args.prompt, args.image_path, type=args.type, beam_size=args.beam_size)
    print("ğŸ“ æ¨ç†ç»“æœ:")
    print(result)