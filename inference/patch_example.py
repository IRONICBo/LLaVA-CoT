#!/usr/bin/env python3
"""
ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„è¿è¡Œæ—¶è¡¥ä¸ç¤ºä¾‹
æä¾›æ›´å®‰å…¨å’Œä¾¿æ·çš„è¡¥ä¸ç®¡ç†
"""

import sys
import os
from contextlib import contextmanager
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from advanced_patch import TransformersRuntimePatcher

@contextmanager
def patched_mllama_processor(custom_file_path=None):
    """
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šè‡ªåŠ¨åº”ç”¨å’Œæ¸…ç† MllamaProcessor è¡¥ä¸
    
    ä½¿ç”¨ç¤ºä¾‹:
        with patched_mllama_processor():
            # åœ¨è¿™é‡Œä½¿ç”¨è¡¥ä¸åçš„å¤„ç†å™¨
            from transformers import AutoProcessor
            processor = AutoProcessor.from_pretrained("model_path")
            # ... è¿›è¡Œæ¨ç†
        # é€€å‡ºæ—¶è‡ªåŠ¨æ¢å¤åŸå§‹å¤„ç†å™¨
    
    Args:
        custom_file_path: è‡ªå®šä¹‰å¤„ç†å™¨æ–‡ä»¶è·¯å¾„
    """
    patcher = TransformersRuntimePatcher()
    patch_applied = False
    
    try:
        # åº”ç”¨è¡¥ä¸
        print("ğŸ”§ æ­£åœ¨åº”ç”¨ MllamaProcessor è¡¥ä¸...")
        patch_applied = patcher.patch_mllama_processor(custom_file_path)
        
        if patch_applied:
            print("âœ… è¡¥ä¸åº”ç”¨æˆåŠŸ")
        else:
            print("âŒ è¡¥ä¸åº”ç”¨å¤±è´¥")
        
        yield patch_applied
        
    finally:
        # è‡ªåŠ¨æ¸…ç†è¡¥ä¸
        if patch_applied:
            print("ğŸ§¹ æ­£åœ¨æ¸…ç†è¡¥ä¸...")
            if patcher.restore_module("transformers.models.mllama.processing_mllama"):
                print("âœ… è¡¥ä¸æ¸…ç†æˆåŠŸ")
            else:
                print("âš ï¸  è¡¥ä¸æ¸…ç†å¤±è´¥")

def example_inference_with_patch():
    """
    ç¤ºä¾‹ï¼šä½¿ç”¨è¡¥ä¸è¿›è¡Œæ¨ç†çš„å®Œæ•´æµç¨‹
    """
    import torch
    from PIL import Image
    
    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿è¡¥ä¸çš„æ­£ç¡®åº”ç”¨å’Œæ¸…ç†
    with patched_mllama_processor() as patch_success:
        if not patch_success:
            print("âŒ æ— æ³•åº”ç”¨è¡¥ä¸ï¼Œé€€å‡º")
            return
        
        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥å’Œä½¿ç”¨ transformers
        from transformers import MllamaForConditionalGeneration, AutoProcessor
        
        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model_name = "Xkev/Llama-3.2V-11B-cot"
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆä½¿ç”¨è¡¥ä¸åçš„ç‰ˆæœ¬ï¼‰
        model = MllamaForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map='cpu',
        ).cuda().eval()
        
        processor = AutoProcessor.from_pretrained(model_name)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨")
        
        # è¿›è¡Œæ¨ç†
        # image = Image.open("your_image.jpg")
        # prompt = "Your question here"
        # ... æ¨ç†ä»£ç 
        
        print("ğŸ¯ æ¨ç†å®Œæˆ")
    
    print("ğŸ ç¨‹åºç»“æŸï¼Œè¡¥ä¸å·²è‡ªåŠ¨æ¸…ç†")

def create_simple_patch_script():
    """
    åˆ›å»ºä¸€ä¸ªç®€å•çš„è¡¥ä¸åº”ç”¨è„šæœ¬
    """
    script_content = '''#!/usr/bin/env python3
"""
ç®€å•çš„è¡¥ä¸åº”ç”¨è„šæœ¬
åœ¨å¯¼å…¥ transformers ä¹‹å‰è¿è¡Œæ­¤è„šæœ¬
"""

import sys
import os
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

try:
    from advanced_patch import patch_mllama_processor
    
    print("ğŸ”§ æ­£åœ¨åº”ç”¨ MllamaProcessor è¡¥ä¸...")
    
    # åº”ç”¨è¡¥ä¸
    success = patch_mllama_processor()
    
    if success:
        print("âœ… è¡¥ä¸åº”ç”¨æˆåŠŸï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥å’Œä½¿ç”¨ transformers åº“")
        print("ğŸ“ ç¤ºä¾‹ä»£ç :")
        print("    from transformers import AutoProcessor")
        print("    processor = AutoProcessor.from_pretrained('model_name')")
    else:
        print("âŒ è¡¥ä¸åº”ç”¨å¤±è´¥")
        sys.exit(1)
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥è¡¥ä¸æ¨¡å—å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿ advanced_patch.py å’Œ processing_mllama.py åœ¨å½“å‰ç›®å½•")
    sys.exit(1)
except Exception as e:
    print(f"âŒ åº”ç”¨è¡¥ä¸æ—¶å‡ºé”™: {e}")
    sys.exit(1)
'''
    
    script_path = Path(__file__).parent / "apply_patch.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    # è®¾ç½®æ‰§è¡Œæƒé™
    os.chmod(script_path, 0o755)
    
    print(f"âœ… åˆ›å»ºè¡¥ä¸è„šæœ¬: {script_path}")
    return script_path

if __name__ == "__main__":
    print("ğŸ¯ è¿è¡Œæ—¶è¡¥ä¸ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºç®€å•çš„è¡¥ä¸è„šæœ¬
    create_simple_patch_script()
    
    print("\nğŸ“– ä½¿ç”¨æ–¹æ³•:")
    print("1. ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ–¹å¼:")
    print("   with patched_mllama_processor():")
    print("       # ä½¿ç”¨è¡¥ä¸åçš„å¤„ç†å™¨")
    print("       pass")
    
    print("\n2. æ‰‹åŠ¨åº”ç”¨æ–¹å¼:")
    print("   from advanced_patch import patch_mllama_processor")
    print("   patch_mllama_processor()")
    
    print("\n3. è„šæœ¬æ–¹å¼:")
    print("   python apply_patch.py")
    print("   # ç„¶ååœ¨åŒä¸€ä¸ª Python ä¼šè¯ä¸­ä½¿ç”¨ transformers")
    
    print("\nğŸ”§ è¿è¡Œç¤ºä¾‹æ¨ç†...")
    # example_inference_with_patch()  # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œç¤ºä¾‹